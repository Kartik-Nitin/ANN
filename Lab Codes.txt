		Lab Codes

1:
	import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

class Perceptron:
    def __init__(self, learning_rate=0.01, n_epochs=100):
        self.learning_rate = learning_rate
        self.n_epochs = n_epochs
        self.weights = None
        self.bias = None
        self.loss_history = []
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for epoch in range(self.n_epochs):
            errors = 0
            for i in range(n_samples):
                z = np.dot(X[i], self.weights) + self.bias
                y_pred = 1 if z >= 0 else 0
                error = y[i] - y_pred
                errors += abs(error)
                self.weights += self.learning_rate * error * X[i]
                self.bias += self.learning_rate * error
            
            self.loss_history.append(errors)
    
    def predict(self, X):
        z = np.dot(X, self.weights) + self.bias
        return np.where(z >= 0, 1, 0)

perceptron = Perceptron(learning_rate=0.01, n_epochs=100)
perceptron.fit(X_train, y_train)

y_pred_train = perceptron.predict(X_train)
y_pred_test = perceptron.predict(X_test)

train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Testing Accuracy: {test_accuracy:.4f}")
print(f"\nConfusion Matrix:\n{confusion_matrix(y_test, y_pred_test)}")
print(f"\nClassification Report:\n{classification_report(y_test, y_pred_test)}")

plt.figure(figsize=(10, 4))
plt.plot(perceptron.loss_history)
plt.xlabel('Epoch')
plt.ylabel('Errors')
plt.title('Perceptron Training Loss')
plt.grid()
plt.show()


2:
	import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.preprocessing import StandardScaler

# ---------------- ACTIVATION FUNCTIONS ----------------
def step(z):
    return np.where(z >= 0, 1, 0)

def sign(z):
    return np.where(z >= 0, 1, -1)

def linear(z):
    return z

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def tanh(z):
    return np.tanh(z)

def relu(z):
    return np.maximum(0, z)

activations = {
    "Step": step,
    "Sign": sign,
    "Linear": linear,
    "Sigmoid": sigmoid,
    "Tanh": tanh,
    "ReLU": relu
}

# ---------------- DATASET ----------------
X, y = load_diabetes(return_X_y=True)
y = np.where(y > y.mean(), 1, 0)

scaler = StandardScaler()
X = scaler.fit_transform(X)

np.random.seed(0)
weights = np.random.randn(X.shape[1])
bias = 1.0

z_data = np.dot(X, weights) + bias

# ---------------- 1. ACTIVATION CURVES ----------------
z = np.linspace(-10, 10, 400)

for name, func in activations.items():
    plt.figure()
    plt.plot(z, func(z))
    plt.title(f"{name} Activation Function")
    plt.xlabel("z (weighted input)")
    plt.ylabel("Output")
    plt.show()




3:
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Reproducibility
tf.random.set_seed(42)

# -------------------- Load Dataset --------------------
data = pd.read_csv(
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv",
    header=None
)

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# -------------------- Train-Test Split --------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -------------------- Feature Scaling --------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# -------------------- Model Builder --------------------
def build_train_evaluate(hidden_layers):
    model = Sequential()
    model.add(Dense(hidden_layers[0], input_dim=8, activation='relu'))

    for neurons in hidden_layers[1:]:
        model.add(Dense(neurons, activation='relu'))

    model.add(Dense(1, activation='sigmoid'))

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    history = model.fit(
        X_train,
        y_train,
        epochs=100,
        batch_size=16,
        validation_split=0.2,
        verbose=0
    )

    y_pred_prob = model.predict(X_test, verbose=0)

    accuracy = model.evaluate(X_test, y_test, verbose=0)[1]
    mse = mean_squared_error(y_test, y_pred_prob)
    r2 = r2_score(y_test, y_pred_prob)

    return accuracy, mse, r2, history

# -------------------- Case Definitions --------------------
cases = {
    "Case 1: 16 (1 HL)": [16],
    "Case 2: 8,8 (2 HL)": [8, 8],
    "Case 3: 8 (1 HL)": [8],
    "Case 4: 16,8 (2 HL)": [16, 8]
}

results = []
histories = {}

# -------------------- Run All Cases --------------------
for case, layers in cases.items():
    acc, mse, r2, hist = build_train_evaluate(layers)
    results.append([case, acc, mse, r2])
    histories[case] = hist
    print(f"{case} -> Accuracy: {acc:.4f}")

# -------------------- Results Table --------------------
results_df = pd.DataFrame(
    results,
    columns=["Architecture", "Accuracy", "MSE", "R2 Score"]
)

print("\nPerformance Comparison Table:\n")
print(results_df.to_string(index=False))

# -------------------- Visualization 1: Accuracy Bar Chart --------------------
plt.figure()
plt.bar(results_df["Architecture"], results_df["Accuracy"])
plt.xlabel("Network Architecture")
plt.ylabel("Test Accuracy")
plt.title("Accuracy Comparison of Feedforward Neural Network Architectures")
plt.xticks(rotation=30)
plt.show()

# -------------------- Visualization 2: Validation Accuracy (ALL CASES) --------------------
plt.figure()

for case, history in histories.items():
    plt.plot(history.history['val_accuracy'], label=case)

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy")
plt.title("Validation Accuracy Comparison for All Architectures")
plt.legend()
plt.show()


4:
	import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# -------------------- 1. Load Dataset --------------------
data = load_breast_cancer()
X = data.data
y = data.target

print("Classes:", data.target_names)
print("Number of features:", X.shape[1])

# -------------------- 2. Train-Test Split --------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -------------------- 3. Feature Scaling --------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# -------------------- 4. Neural Network --------------------
model = Sequential([
    Dense(32, activation='tanh', input_shape=(30,)),
    Dense(16, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# -------------------- 5. Training --------------------
print("\nTraining model...")
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
print("Training completed.")

# -------------------- 6. Predictions --------------------
y_probs = model.predict(X_test)
y_pred = (y_probs > 0.5).astype(int).flatten()

# -------------------- 7. Evaluation --------------------
print("\n==============================")
print("CLASSIFICATION REPORT")
print("==============================")
print(classification_report(y_test, y_pred, target_names=data.target_names))
# Print accuracy as a percentage
accuracy_pct = (y_pred == y_test).mean() * 100
print(f"Accuracy: {accuracy_pct:.2f}%")

cm = confusion_matrix(y_test, y_pred)

plt.figure()
plt.imshow(cm)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks([0, 1], data.target_names)
plt.yticks([0, 1], data.target_names)

for i in range(2):
    for j in range(2):
        plt.text(j, i, cm[i, j], ha="center", va="center")

plt.show()
